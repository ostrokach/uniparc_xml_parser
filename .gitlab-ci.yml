default:
  image: condaforge/linux-anvil-cos7-x86_64:latest

stages:
  - build
  - test
  - deploy

# === Variables ===

variables:
  PACKAGE_VERSION: 0.2.0

# === Configurations ===

.skip-custom-pipelines:
  except:
    variables:
      - $UPDATE_TABLES

.configure:
  extends:
    - .skip-custom-pipelines
  before_script:
    # Rust
    - curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
    - source $HOME/.cargo/env
    # Conda
    - |
      cat <<EOF > ~/.condarc
      channel_priority: strict
      channels:
        - conda-forge
        - ostrokach-forge
        - defaults
      EOF
    - source /opt/conda/etc/profile.d/conda.sh
    - conda activate base
    - conda update -yq conda

# === Build ===

build:
  stage: build
  extends:
    - .configure
  script:
    - mkdir -p "${CI_PROJECT_DIR}/conda-bld"
    - conda build "${CI_PROJECT_DIR}/.conda" --output-folder "${CI_PROJECT_DIR}/conda-bld"
  artifacts:
    paths:
      - conda-bld

# === Test ===

test:
  stage: test
  extends:
    - .configure
  dependencies:
    - build
  script:
    # Create conda environment for testing
    - conda create -n test -q -c file://${CI_PROJECT_DIR}/conda-bld "python=3.9" ${CI_PROJECT_NAME}
    - conda activate test
    # Run tests
    - uniparc_xml_parser --help
    # - python -m pytest -c setup.cfg --color=yes "tests/"
    # Save binary for later
    - mkdir package/
    - cp $(which uniparc_xml_parser) package/
  artifacts:
    paths:
      - package/

# download:
#   stage: download
#   script:
#     - 'wget --header="JOB-TOKEN: $CI_JOB_TOKEN" ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/my_package/0.0.1/file.txt'

# === Pages ===

.pages:
  stage: test
  extends:
    - .configure
  dependencies:
    - build
  script:
    # Install requirements
    - conda create -n test -q -c file://${CI_PROJECT_DIR}/conda-bld "python=3.9"
    - conda activate test
    - python -m pip install -r docs/requirements.txt
    # Build docs
    - mkdocs build docs
  dependencies:
    - build
  artifacts:
    paths:
      - public

# === Deploy ===

deploy-cargo:
  stage: deploy
  extends:
    - .configure
  script:
    - cargo publish --no-verify
  dependencies: []
  only:
    - tags

deploy-conda:
  stage: deploy
  extends:
    - .configure
  script:
    - anaconda -t $ANACONDA_TOKEN upload $CI_PROJECT_DIR/conda-bld/*/*.tar.bz2 -u ostrokach-forge --no-progress
  dependencies:
    - build
  only:
    - tags

deploy-package:
  stage: deploy
  extends:
    - .configure
  script:
    - >
      curl --header "JOB-TOKEN: $CI_JOB_TOKEN" --upload-file $CI_PROJECT_DIR/package/uniparc_xml_parser
      "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/${CI_PROJECT_NAME}/${PACKAGE_VERSION}/uniparc_xml_parser"
  dependencies:
    - test
  only:
    - tags

# === Run pipeline ===

run-pipeline:
  image: ubuntu:20.04
  before_script:
    # Install global dependencies
    - apt-get update -y -qq -o=Dpkg::Use-Pty=0
    - apt-get install -y -qq -o=Dpkg::Use-Pty=0 curl gettext-base gzip openssl rsync
    # Install ssh client
    - "which ssh-agent || ( apt-get install -y -qq -o=Dpkg::Use-Pty=0 openssh-client -y )"
    - eval $(ssh-agent -s)
    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - echo "$KNOWN_HOSTS" >> ~/.ssh/known_hosts
    # Test that ssh client works
    - ssh strokach@conda-envs.proteinsolver.org "echo hello"
    # Install gcloud
    - echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
    - apt-get install -y -qq -o=Dpkg::Use-Pty=0 apt-transport-https ca-certificates gnupg
    - curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -
    - apt-get update -y -qq -o=Dpkg::Use-Pty=0
    - apt-get install -y -qq -o=Dpkg::Use-Pty=0 google-cloud-sdk
    - gcloud auth activate-service-account --key-file="${GCLOUD_SERVICE_ACCOUNT_FILE}"
    # Install conda
    - |
      cat <<EOF > ~/.condarc
      channel_priority: strict
      channels:
        - conda-forge
        - ostrokach-forge
        - defaults
      EOF
    - curl -s -L https://github.com/conda-forge/miniforge/releases/download/4.9.2-5/Mambaforge-4.9.2-5-Linux-x86_64.sh > miniconda.sh
    - openssl dgst -sha256 miniconda.sh | grep 7f0ad0c2f367751f7878d25a7bc1b4aa48b8dcea864daf9bc09acb595102368b
    - sh miniconda.sh -b -p /opt/conda
    - source /opt/conda/etc/profile.d/conda.sh
  script:
    - conda activate base
    - mamba install 'python=3.9' pyarrow uniparc_xml_parser
    - mkdir output
    - cd output
    - >
      curl -sS ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/uniparc/uniparc_all.xml.gz
      | head -c 100000000
      | zcat
      | uniparc_xml_parser || true
    # Convert to Parquet files
    - python ../scripts/csv_to_parquet.py -f uniparc.tsv -c uniparc_id,sequence,sequence_length,sequence_checksum
    - python ../scripts/csv_to_parquet.py -f domain.tsv -c uniparc_id,database,database_id,interpro_name,interpro_id,domain_start,domain_end
    - python ../scripts/csv_to_parquet.py -f xref.tsv -c uniparc_id,xref_id,db_type,db_id,version_i,active,version,created,last
    - for property in component gene_name ncbi_gi ncbi_taxonomy_id pdb_chain protein_name proteome_id uniprot_kb_accession ; do
      echo ${property} ;
      python ../scripts/csv_to_parquet.py -f ${property}.tsv -c uniparc_id,xref_id,property,value ;
      done
    # Upload Parquet files to our server
    - rsync -rv --info=progress2 -p --chmod=ug=rwX,o=rX *.parquet strokach@conda-envs.proteinsolver.org:/share/data/uniparc/
    # Update BigQuery tables
    - bq load --source_format=PARQUET --replace --clustering_fields uniparc_id ostrokach-data:uniparc.uniparc uniparc.parquet
    - bq load --source_format=PARQUET --replace --clustering_fields database,uniparc_id ostrokach-data:uniparc.domain domain.parquet
    - bq load --source_format=PARQUET --replace --clustering_fields db_type,uniparc_id,xref_id ostrokach-data:uniparc.xref xref.parquet
    - for property in component gene_name ncbi_gi ncbi_taxonomy_id pdb_chain protein_name proteome_id uniprot_kb_accession ; do
      echo ${property} ;
      bq load --source_format=PARQUET --replace --clustering_fields uniparc_id,xref_id ostrokach-data:uniparc.${property} ${property}.parquet ;
      done
  only:
    variables:
      - $UPDATE_TABLES
